{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a7edac-0c36-4717-9080-abd8d2d41228",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial for LEAP-REU data processing and visualization\n",
    "\n",
    "**Authors: Yu Huang, Sungduk Yu**\n",
    "\n",
    "GitHub repository links: [tutorials for REU dataset](https://github.com/sungdukyu/LEAP_REU_Dataset_Notebook); [LEAP-REU23 Bootcamp](https://github.com/leap-stc/LEAP-bootcamps).\n",
    "\n",
    "It is an introductory tutorial for a demo dataset from the climate model [E3SM-MMF](https://www.exascaleproject.org/research-project/e3sm-mmf/). See [E3SM-MMF_baseline](https://github.com/sungdukyu/E3SM-MMF_baseline/tree/main) and [E3SM](https://e3sm.org/wp-content/uploads/2021/11/E3SM_Brochure-2021.pdf) for more information.\n",
    "\n",
    "This notebook includes preprocessing of the unstructured data, which cannot be managed by xarray directly, and visualization of climate variables. \n",
    "\n",
    "The goal is to practice the skills covered during Week1 of the bootcamp to carry out climate analysis, and to get more familiar with the climate concepts using the REU dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67a231-8690-44a5-958f-df77666d6819",
   "metadata": {},
   "source": [
    "## Git Authentification\n",
    "\n",
    "Use terminal or the left side bar to push your files if you want to use Git to keep track of your file.\n",
    "\n",
    "Run the below code to give LEAP-Pangeo access to your Github account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bf2a5-82ad-44ba-9b31-540fcfdc5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gh_scoped_creds\n",
    "# %ghscopedcreds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900db437-076c-442f-ac42-1d2305d96c48",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Install and load python packages\n",
    "\n",
    "We use [mamba](https://mamba.readthedocs.io/en/latest/installation.html) (instead of conda) to install packages on Hub. Please click the \"+\" button on the leftup corner to launch a terminal, copy the below commands after \"!\" and run them on the terminal if you cannot directly import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce2d01-64b1-49b1-bf98-dcb7c438a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mamba install -y pynco pynio pyngl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a90b28-ae17-4b53-a27c-d5d91f63d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import cftime\n",
    "from nco import Nco\n",
    "from tqdm import tqdm\n",
    "import Ngl\n",
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f0d33-a966-4ff0-835b-a67c09c97143",
   "metadata": {},
   "source": [
    "### Load dataset from Google Cloud\n",
    "\n",
    "#### Open Google Cloud Storage File System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409fe2e-dc2d-4b10-ad7d-96a43c3316d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295c973-cff2-4726-8f71-23ad58692013",
   "metadata": {},
   "source": [
    "#### List files in the bucket where the E3SM-MMF dataset is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff2b20-4f16-465f-973e-704570beb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"gs://leap-persistent-ro/sungdukyu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86cd36-01af-4e9f-b897-c2b9b4eb8e81",
   "metadata": {},
   "source": [
    "#### Open the file you want using xarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404018f2-86b0-42b0-a058-11936f3e4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fs.get_mapper('leap-persistent-ro/sungdukyu/E3SM-MMF_ne4.train.output.zarr')\n",
    "ds = xr.open_dataset(mapper, engine='zarr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f01e5-1cd5-4fd5-90a8-3554d366625b",
   "metadata": {},
   "source": [
    "#### Check which variables are included & their dimensions/shapes \n",
    "\n",
    "Use [E3SM-MMF Dataset Variable List](https://docs.google.com/spreadsheets/d/1ljRfHq6QB36u0TuoxQXcV4_DSQUR0X4UimZ4QHR8f9M/edit#gid=0) to check the physical meaning of each variable.\n",
    "\n",
    "Check the original data coordinates first. Instead of using time, latitude, longitude as the coordinates, the raw data uses **sample**(time step) and **ncol**(column index).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb1813-85b6-4cd2-8afa-13e1c67e22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8878db-d670-47d4-9616-d8bff76c973d",
   "metadata": {},
   "source": [
    "### Reorganize the temporal dimension/coordinate\n",
    "\n",
    "#### Add the *time* dimension  \n",
    "Originally the time information is coded in the variables **ymd** and **tod**. The **sample** index represents the time step count. \n",
    "\n",
    "**ymd** includes date information: the first digit indicates the index of year, the next two digits indicate the month and the last three digits indicates the calendar day in the year.\n",
    "\n",
    "**tod** represents time in the day counted in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d02143-3357-452b-8c53-0f1462de8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ymd.values[0], ds.tod.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcfce2-16ed-4a1b-afe3-87a6fbc29fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all sample points\n",
    "year=ds['ymd']//10000\n",
    "month=ds['ymd']%10000//100\n",
    "day=ds['ymd']%10000%100\n",
    "hour=ds['tod']//3600\n",
    "minute=ds['tod']%3600//60\n",
    "\n",
    "k=0\n",
    "t = []\n",
    "for k in range(len(ds['ymd'])):\n",
    "    t.append(cftime.DatetimeNoLeap(year[k],month[k],day[k],hour[k],minute[k]))\n",
    "\n",
    "# add the time array to the 'sample' dimension; then, rename\n",
    "ds['sample'] = t\n",
    "ds = ds.rename({'sample':'time'})\n",
    "\n",
    "# now 'time' dimension replaced 'sample' dimension.\n",
    "ds = ds.drop(['tod','ymd'])\n",
    "\n",
    "# Check the current **time** dimension, read the timestep\n",
    "ds.time.values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cdd8b-641d-4f4d-b830-52c48fd48a97",
   "metadata": {},
   "source": [
    "#### Reduce the data size/coarse the time resolution \n",
    "\n",
    "The **time** dimension is large. For avoid memory issues, currently we'll keep only one sample each day. \n",
    "\n",
    "You can take the daily/monthly mean/max/min data or the data at a specific time each day. \n",
    "\n",
    "There are many ways to do that. You can uncomment the codes to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb795b4-d598-4781-9e8c-c04ac1ff5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the original data at 20-min time step and gridded by column index\n",
    "ds_origin = ds.copy()\n",
    "\n",
    "# 1. tried to resample the whole dataset but then the memory blows up\n",
    "# ds = ds.resample(time='1D').mean('time')\n",
    "# ds\n",
    "\n",
    "# 2. select data at noon per day for the whole dataset \n",
    "# itime = np.arange(36,len(ds.time),24*3)\n",
    "# ds = ds.isel(time = itime)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114b59f-e78a-4a74-9071-de3eb384a2c9",
   "metadata": {},
   "source": [
    "I can continue taking the monthly mean using *resample* function, which will further reduce the data size. It requires more memory so I only process two 2D variables here *total precipitation* and *net surface shortwave radiation*. For the sake of convenience, I will loosely refer to these two variables as 'precipitation' and 'radiation' in the later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ce993-55a2-45ef-a7ac-9def342bc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the data to be monthly instead of daily\n",
    "# first do the transformation so we have: total precipitation = snow + rainfall\n",
    "\n",
    "ds['cam_out_PRECT'] = ds['cam_out_PRECC'] + ds['cam_out_PRECSC']\n",
    "ds = ds[['cam_out_PRECT', 'cam_out_NETSW']].resample(time='1M').mean(dim='time')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114385a-f7d6-4379-b072-e434dea2ad27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remap and interpolate the data so that they have structured lat, lon coordinates\n",
    "\n",
    "#### Open a file that stores grid information, and check the original lat, lon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f70994-cf08-4039-99a6-4092ff45c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid info\n",
    "mapper = fs.get_mapper(\"gs://leap-persistent-ro/sungdukyu/E3SM-MMF_ne4.grid-info.zarr\")\n",
    "ds_grid = xr.open_dataset(mapper, engine='zarr')\n",
    "ds_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870d802-cee8-4f3d-9a14-e1b4988c760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ds_grid['lat'].plot(label='lat', ax=ax)\n",
    "ds_grid['lon'].plot(label='lon', ax=ax)\n",
    "ax.set_ylabel('deg')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c451630-fd74-4d23-a949-bb7c05ed56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('column number is ',len(np.unique(ds_grid.ncol.values)))\n",
    "print('if each lat occured in the data can be paired with a lon, the (lat,lon) grid number is', len(np.unique(ds_grid.lat.values))*len(np.unique(ds_grid.lon.values)))\n",
    "np.unique(ds_grid.lon.values.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3527f-9963-411c-a219-218c816402ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Change the geo-coordinate from column index to multi-index (lat, lon) and see what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545833d-346c-450f-b0b2-451e573bb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original lat and lon info\n",
    "lat = ds_grid.lat.values.round(2) \n",
    "lon = ds_grid.lon.values.round(2) \n",
    "\n",
    "# merge the original grid info with the dataset containing atmos variables\n",
    "ds_multiindex = ds.copy()\n",
    "ds_multiindex['lat'] = (('ncol'),lat.T) # (('sample', 'ncol'),lat.T)\n",
    "ds_multiindex['lon'] = (('ncol'),lon.T)\n",
    "\n",
    "# set multi-index for the original dataset using lat and lon\n",
    "ds_multiindex = ds_multiindex.set_index(index_id=[\"lat\", \"lon\"])\n",
    "index_id = ds_multiindex.index_id\n",
    "ds_multiindex = ds_multiindex.drop('index_id')\n",
    "ds_multiindex = ds_multiindex.rename({'ncol':'index_id'})\n",
    "ds_multiindex = ds_multiindex.assign_coords(index_id = index_id)\n",
    "ds_multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faaf130-2b67-4e59-92a7-87468ffb285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with stacked (lat, lon) grids in the original dataset, all values are NaNs\n",
    "time = ds.time.values\n",
    "\n",
    "data_np = np.empty(shape=(len(time), len(np.unique(lat)),len(np.unique(lon))))\n",
    "data_np[:,:] = np.nan\n",
    "\n",
    "ds_latlon = xr.Dataset(\n",
    "     data_vars={\n",
    "         # v: ((\"time\",\"index_id\"), np.zeros([len(time), len(np.unique(lat))*len(np.unique(lon))]))\n",
    "         v: ((\"time\",\"lat\",\"lon\"), data_np)\n",
    "         for v in ['cam_out_NETSW','cam_out_PRECT']\n",
    "     },\n",
    "     coords={\n",
    "         \"time\": ds.time,\n",
    "         # \"index_id\": pd.MultiIndex.from_product(\n",
    "         #    [np.unique(lat), np.unique(lon)], names=[\"lat\", \"lon\"],),\n",
    "         \"lat\": np.unique(lat),\n",
    "         \"lon\": np.unique(lon),\n",
    "         # \"lev\": ds.lev,\n",
    "    },\n",
    ")\n",
    "\n",
    "# use multi-index so that we can assign the column data to the (lat,lon) data\n",
    "ds_latlon = ds_latlon.stack(index_id=['lat','lon'])\n",
    "ds_latlon \n",
    "\n",
    "# print(len(ds_multiindex.index_id.values))\n",
    "# ds_latlon.sel(index_id=(-32.59, 320.27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faf073-b3a7-4d88-b5d5-dfda278af4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tqdm to visualize the progress of the below loop\n",
    "# this cell takes about 20min to run, please patiently wait when it is run\n",
    "ds_precc = ds_latlon.cam_out_PRECT.copy()\n",
    "ds_netsw = ds_latlon.cam_out_NETSW.copy()\n",
    "\n",
    "for i in tqdm(ds_multiindex.index_id.values):\n",
    "    # ds_latlon.loc[{\"index_id\": i}] = ds_multiindex[[' cam_out_NETSW','cam_out_PRECC']].sel(index_id = i) \n",
    "    #### wrong, will lead to all vars have the same values\n",
    "    ds_precc.loc[{\"index_id\": i}] = ds_multiindex['cam_out_PRECT'].sel(index_id = i)\n",
    "    ds_netsw.loc[{\"index_id\": i}] = ds_multiindex['cam_out_NETSW'].sel(index_id = i)\n",
    "\n",
    "ds_latlon['cam_out_PRECT'] = ds_precc.copy()\n",
    "ds_latlon['cam_out_NETSW'] = ds_netsw.copy()\n",
    "\n",
    "ds_unstack = ds_latlon.unstack('index_id')\n",
    "ds_unstack\n",
    "\n",
    "## if we directly visualize the 2D maps, then there are many missing values \n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ds_unstack. cam_out_NETSW.mean('time').plot(cmap='RdBu_r',ax=ax[0])\n",
    "ds_unstack.cam_out_PRECT.mean('time').plot(cmap='RdBu_r',ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df14de-caca-4d1e-bd25-c6886ea67860",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\"> **Look, the raw dataset is visualized weirdly directly using xarray because it is on an unstructured grid and xarray cannot handle these coordinate points properly. See [page 9](https://www.osti.gov/servlets/purl/1807356) to know more about the raw grid setup.**</span>\n",
    "\n",
    "#### What does the raw grid look like?\n",
    "\n",
    "Use [pynco](https://pynco.readthedocs.io/en/stable/) to remap data on the unstructured grid to a structured grid. Do not bother yourself to try to understand this kind of grid right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3829627-661e-437e-8058-49f438672f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = '../E3SM-MMF_ne4_train_output_monthly_raw.nc'\n",
    "outfile = '../E3SM-MMF_ne4_train_output_monthly_remap.nc'\n",
    "#Mapfile to convert unstructured data to gridded data\n",
    "mapfile = '../map_ne4pg2_to_180x360_aave.20220722.nc'\n",
    "\n",
    "ds.to_netcdf('../E3SM-MMF_ne4_train_output_monthly_raw.nc')\n",
    "\n",
    "nco = Nco()\n",
    "nco.ncks(input=infile, output=outfile, map=mapfile)\n",
    "\n",
    "ds_remap = xr.open_dataset(outfile)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ds_remap['cam_out_NETSW'].mean('time').plot(cmap='RdBu_r',ax=ax[0])\n",
    "ds_remap['cam_out_PRECT'].mean('time').plot(cmap='RdBu_r',ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986b8fe-d8e8-4f5d-925b-1b158f3eec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_remap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f5dbd-791d-4a2a-abd3-465e809ac3ed",
   "metadata": {},
   "source": [
    "#### Interpolate the unstructured data at 2-deg resolution\n",
    "\n",
    "For simplicity and memory consideration, we use a tool [PyNGL](https://www.pyngl.ucar.edu/Functions/Ngl.natgrid.shtml) to interpolate the **RAW** data to make it structured on the grid we want, so that we can make some climate analysis using the skills you learned earlier this week. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5201eb-9831-4b88-b0f2-824e75f2290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original lat and lon info\n",
    "ncol = ds.ncol.values\n",
    "\n",
    "# new lat and lon grids that we finally want\n",
    "nlat = np.arange(-90, 90.5, 2)\n",
    "nlon = np.arange(0, 360, 2)\n",
    "\n",
    "# create a nan-value xr dataset to contain new remapped & interpolated data \n",
    "data_var = np.empty([len(time), len(nlat), len(nlon)])\n",
    "data_var[:,:,:] = np.nan\n",
    "\n",
    "ds_new = xr.Dataset(\n",
    "     data_vars={\n",
    "         v: ((\"time\",\"lat\",\"lon\"), data_var)\n",
    "         for v in ['cam_out_NETSW','cam_out_PRECT']\n",
    "     },\n",
    "     coords={\n",
    "         \"time\": ds.time,\n",
    "         \"lat\": nlat,\n",
    "         \"lon\": nlon,\n",
    "        # \"lev\": ds.lev,\n",
    "    },\n",
    ")\n",
    "\n",
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad4d8a-7e5d-4232-8e8e-9527b9174638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prect = data_var.copy()\n",
    "data_netsw = data_var.copy()\n",
    "\n",
    "# interpolate the data using pyngl\n",
    "# optional: use multiprocessing to save the running time\n",
    "\n",
    "for it, tt in enumerate(ds.time):\n",
    "    data = ds.sel(time=tt).cam_out_NETSW.values\n",
    "    iarr = Ngl.natgrid(lat, lon, data, nlat, nlon) #.squeeze()\n",
    "    data_netsw[it,:] = iarr\n",
    "    \n",
    "    data = ds.sel(time=tt).cam_out_PRECT.values\n",
    "    iarr = Ngl.natgrid(lat, lon, data, nlat, nlon) #.squeeze()\n",
    "    data_prect[it,:] = iarr\n",
    "\n",
    "ds_new['cam_out_NETSW'].values = data_netsw\n",
    "ds_new['cam_out_PRECT'].values = data_prect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e58b72-09c6-4caa-adf6-ecebab7d8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the maps of these two variables at a specific time step\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ds_new['cam_out_NETSW'].isel(time=10).plot(cmap='RdBu_r',ax=ax[0])\n",
    "ds_new['cam_out_PRECT'].isel(time=10).plot(cmap='RdBu_r',ax=ax[1])\n",
    "ax[0].set_title('NET SW Surface')\n",
    "ax[1].set_title('Total Precip (m/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b297b4-5026-4088-ae94-c24391004629",
   "metadata": {},
   "source": [
    "The maps looks smoother and closer to the realistic condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de1451-d4cf-42e1-bb1a-0382b93850d4",
   "metadata": {},
   "source": [
    "## Now, analysis and visualization can be done using xarray\n",
    "\n",
    "<span style=\"color:blue\"> Work in a group of 2, each group should finish two sections of analysis. Please use the plotting skills you learned to make the figures look well-annotated, nice and clear. Use Google or ChatGPT if you have questions regarding the climate concepts. </span>\n",
    "    \n",
    "### [Analysis 1] Time series and trend\n",
    "\n",
    "We can use the original dataset to calculate the *global mean* (spatial mean) time series. The unit for precipitation is m/s. \n",
    "\n",
    "Note it should be the weighted average mean based on the area of each atmos grid/column. Here we provide you the grid area data so you do not need to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9093d-fba5-45e4-9fd5-3c170e0c419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the data whose dimension size was not reduced\n",
    "ds_origin = xr.merge([ds_grid, ds_origin])\n",
    "\n",
    "# total precipitation = rainfall + snowfall\n",
    "PRECT = ds_origin['cam_out_PRECC'] + ds_origin['cam_out_PRECSC']\n",
    "\n",
    "# area-weighted global mean PRECT\n",
    "# required concept: avg weights, broadcast, resampling\n",
    "PRECT_mean = (PRECT * (ds_origin['area']/ds_origin['area'].sum())).sum('ncol')\n",
    "PRECT_mean_daily = PRECT_mean.resample(time='1D').mean('time')\n",
    "PRECT_mean_monthly = PRECT_mean.resample(time='1M').mean('time')\n",
    "\n",
    "# visualization\n",
    "fig, ax = plt.subplots()\n",
    "PRECT_mean.plot(label='instantaneous', ax=ax)\n",
    "PRECT_mean_daily.plot(label='daily mean', ax=ax)\n",
    "PRECT_mean_monthly.plot(label='monthly mean', ax=ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8773768-ce5b-4b8c-ad46-b89e43c59f1f",
   "metadata": {},
   "source": [
    "Or, we can use use the processed dataset on structured lat/lon coordinates. In this case, we need to calculate the area of each 2x2 degree grid. You'll only be able to show the time series of monthly mean data or annual data, because we've resampled it at the monthly frequency.\n",
    "\n",
    "Please show the time series of global weighted mean cam_out_NETSW using the processed dataset *ds_new*. Refer your Assignment #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40376630-1bdc-4a25-939e-7a3c0afdfae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "weights  = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3fb57-6e58-4d1b-aca3-ecd8d6a2f847",
   "metadata": {},
   "source": [
    "Do you see a trend of the precip & radiation data? What else do you find with the time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087b1b6-1354-4c0d-9dcb-da693d6c4e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "468a6de4-d68f-4888-a5f3-d6b0fa512223",
   "metadata": {},
   "source": [
    "#### Spatial variability of monthly precipitation \n",
    "\n",
    "For the same month in some regions, like United States, precipitation can be very unevenly distributed - some locations can be extremely dry and some can be extremely wet. \n",
    "\n",
    "Use ds_new in the previous analysis and show the variability of monthly precipitation in United States by plot the time series of the median values and the mean $\\pm 1$ std values across all grids in  [120W, 70W] [24N, 50.5N]. \n",
    "\n",
    "Hint: your x axis should be time (the time step is one month), and y axis should be the precipitation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecb969-7146-48e6-947b-1681f5e1f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "# If you want to take weighted mean, you also need to apply weights to the standard deviation\n",
    "US_precip = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3488890-e9ed-4937-9078-37e645104c5e",
   "metadata": {},
   "source": [
    "### [Analysis 2] Histograms/distributions\n",
    "\n",
    "Please use PRECT (the variable you created in Analysis 1) and ds_new. \n",
    "\n",
    "Convert the unit of precipitation from m/s to mm/hr or mm/day, and plot:\n",
    "\n",
    "1. a histogram for monthly precipitation from all grids across the globe; \n",
    "2. a histogram for daily precipitation from all grids \n",
    "\n",
    "to show the statistical distributions of the variable values. Use log scale for y axis. You can plot them on the same graph or do it separately. \n",
    "\n",
    "See [xarray.plot.hist](https://docs.xarray.dev/en/stable/generated/xarray.plot.hist.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ab069-a44f-4052-b108-18aff2ed188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "# m/s = ? mm/hr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5f646-f353-4cd4-b1bd-8ecb463fea64",
   "metadata": {},
   "source": [
    "Simply describe what you see, especially the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a8eb0-6222-4238-8235-065ffd29c8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38c894f-3c0c-43c3-bfc1-f52a6acc84c3",
   "metadata": {},
   "source": [
    "### [Analysis 3] Climatology, Anomalies and Normalization\n",
    "\n",
    "#### 2D horizontal maps\n",
    "\n",
    "**<span style=\"color:blue\"> We'll use *ds_new* for the rest analysis from here unless specified.</span>**\n",
    "\n",
    "Review the lecture on Day 2, read **[Climatology vs weather](https://drought.unl.edu/Education/DroughtIn-depth/WhatisClimatology.aspx)** and **[Current Climate](https://climateknowledgeportal.worldbank.org/country/united-states/climate-data-historical)** to understand what is climatology - naively it can be interpreted as the average for weather in an area over decades. In other words, climatology is what would a typical year / the average condition be like for climate variables, such as precipitation or temperature, in a region or over the globe. It can be the seasonal cycles or just annual mean condition, depending on your goal.\n",
    "\n",
    "Choose a proper projection (use cartopy) and visualize the 2D maps of climatology status for total precipitation and surface net SW using subplot. Choose a nice colormap and annotate the information properly.\n",
    "\n",
    "Hint: it can be achieved by simply taking temporal mean. **You are not yet required to deal with seasonal cycles (namely, don't group data by month)** in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918db6e1-bd7d-4768-b72f-bcbf76946c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9959f3-23cb-4af7-9a81-da7f5287d3bd",
   "metadata": {},
   "source": [
    "#### Zonal mean climatology\n",
    "Other than plotting the 2D maps, we can also plot curves of the zonal mean radiation/precipitation. \n",
    "\n",
    "Hint: Plot the curve along latitude, with x-axis as lat and y-axis as the zonnal mean variables. You can plot radiation and precipitation in two subplots or plot them together using **[twin axes](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/two_scales.html#sphx-glr-gallery-subplots-axes-and-figures-two-scales-py)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09a451-f02d-4604-9926-1214f4eda7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67cb3f-7daf-4f14-a2a0-e8ccbb2b0d76",
   "metadata": {},
   "source": [
    "Simply describe the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51dc73-cebb-49d6-bfde-f2d3bbf6395f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55bc2613-f420-4375-a7ff-2607deaf5a7d",
   "metadata": {},
   "source": [
    "#### Anomalies and standard anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbf976-260d-4a53-b38b-dfecbc2f14fd",
   "metadata": {},
   "source": [
    "What is anomaly in climate sciences? Read **[Temperature Anomalies](https://www.ncei.noaa.gov/access/monitoring/dyk/anomalies-vs-temperature)** to learn more. Anomalies can be visualized as time series plots or maps. \n",
    "\n",
    "When you want to compare the variability of two variables whose original magnitudes are different, it often helps to express the data in terms of **[Standardized Anomalies](https://iridl.ldeo.columbia.edu/dochelp/StatTutorial/Climatologies/index.html)**. \n",
    "\n",
    "Visualize the time series of standardized anomalies for *global mean* precipitation & radiation on the same plot. Take a rolling mean with time window as 3-month to make the curves smoother. Show the legend properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae64fbd-2b4b-4857-ba9d-319d09111e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1efbc-0f3c-4908-84ca-9087ffd46567",
   "metadata": {},
   "source": [
    "Did you find any pattern? Which variable has stronger variability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbbe24-e3da-4bd0-80cf-0a24482e796d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d84433aa-3c59-4f60-b884-137ee706660f",
   "metadata": {},
   "source": [
    "### [Analysis 4] Seasonality\n",
    "\n",
    "We've seen that the data exhibits strong seasonality/periods in the previous analysis. We can further visualize the seasonal cycles from the perspective of climatology, using *Groupby* function.\n",
    "\n",
    "Groupby the *global mean* precipitation from ds_new in by month and take the mean values. Plot precipitation respect to month. \n",
    "\n",
    "Compare with plots in [paper for net SW surface](https://www.researchgate.net/figure/Climatological-mean-annual-cycle-of-a-net-shortwave-radiation-b-net-longwave-radiation_fig4_314121791) and [paper for precip](https://www.researchgate.net/figure/Mean-seasonal-cycle-of-a-temperatures-over-the-globe-the-tropical-ocean-and-the_fig1_234072312).\n",
    "\n",
    "Hint: the x-axis should be Jan, Feb, â€¦ Dec; and y-axis should be the groupby mean of *global mean* precipitation for each month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cd79d-ece4-4cc4-864c-f8d583c62d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea3d10-1a85-494e-817d-e1fb2f2d2c92",
   "metadata": {},
   "source": [
    "We can also groupby the data by seasons, sush as JJA (Jun, Jul, Aug) or DJF (Dec, Jan, Feb).\n",
    "\n",
    "Plot a map of radiation difference in summer and winter over the whole globe (e.g. JJA vs DJF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3212a-6fd4-445e-8a37-7bef595bfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "\n",
    "season_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d676d1-0cb5-4ab5-be5e-45249b5b1e79",
   "metadata": {},
   "source": [
    "What did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210a31d-3048-4aef-b634-dc58f925e2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01109390-0528-4514-af2c-e732645afd81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Analysis 5] Vertical profiles of the 3D variables\n",
    "\n",
    "#### Interpolation of data\n",
    "\n",
    "Danger: <span style=\"color:red\">restart your server and select memory = 64G or 128G option for this analysis. Only do this when you process this task.</span>\n",
    "\n",
    "Till now, we only visualize and process a few 2D variables, we can also create an interpolated dataset/dataarray for the 3D variable **state_t**, air temperature, with *lev* dimension following the data processing steps to generate ds_new. \n",
    "\n",
    "In order to avoid requesting huge memory, we'll only keep the 3D data at every 4th (try 6th when the current memory size is still not sufficient) lev coordinate points and resample it to be monthly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9854b4-21ff-4cb3-adc0-7a4cd53b975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "\n",
    "lev = ds_origin.lev\n",
    "lev = lev[0::4]\n",
    "ds_3d = ds_origin.sel(lev=lev)\n",
    "\n",
    "# try to release some memory by deleting useless datasets if needed\n",
    "# del ds_origin, ds, ds_grid \n",
    "\n",
    "# memory failure: ds_3d.resample(time='1M').mean(dim='time')\n",
    "ds_3d = ds_3d.state_t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4ef9b-66b8-41d6-ba37-8641610d7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "# create a nan-value xr dataset to contain new remapped & interpolated data \n",
    "data_var = np.empty([len(time), len(lev), len(nlat), len(nlon)])\n",
    "data_var[:,:,:,:] = np.nan\n",
    "\n",
    "ds_new3d = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ba6b1-40ab-4783-9e9f-90d480e2b291",
   "metadata": {},
   "source": [
    "#### Vertical structure of state_t from any grid \n",
    "\n",
    "Select any one grid (or domain) as you want and take the temporal mean (you also need to take the horizontal mean if you select a domain instead one grid before plotting). \n",
    "\n",
    "Plot the state_t values along the altitude/height level. Your x-axsis should be the values of state_t, and the y-axis should be lev. \n",
    "\n",
    "Compare your plots with [this website](https://crisp.nus.edu.sg/~research/tutorial/atmos.htm#:~:text=The%20vertical%20profile%20of%20the,%2C%20mesopause%20and%20thermopause%2C%20respectively.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db53e1e-b82f-4bb2-963e-6194a3225b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c7d04-444e-4391-914c-9ef920d889aa",
   "metadata": {},
   "source": [
    "Which level (lev=0 or lev=9), do you think, is closer to the earth surface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b321317-f16a-4965-9b32-cf375ef2d80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb86061d-7799-4f37-98a1-5fc626eb67e8",
   "metadata": {},
   "source": [
    "#### Seasonality or distribution across the latitudes\n",
    "\n",
    "The vertical profiles of state_t from a domain/grid can show seasonal variation and can also exhibit geographic characteristics respect to the latitude. \n",
    "\n",
    "Make ONE plot to show Either the seasonal change or geographic characteristics of the air temperature.\n",
    "\n",
    "Hint: one way to visualize the seasonal change is to take the JJA and DJF mean of domain mean state_t, and plot these two vertical curves on the same figure (for the figure, x-axis: stat_t, y_axis: lev). \n",
    "\n",
    "Hint: one way to visualize the geographic difference is to take the temporal and zonal mean of the state_t, so for each lat and lev, you have one state_t value (for the figure, x-axis: lat, y_axis: lev, color: state_t). \n",
    "\n",
    "You do not need to make the figure exactly following these steps. Please unleash your creativity and it will be fine as long as you plot something that makes sense and helps understand the concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111f39e-0222-49e3-9a77-f4a0e10a23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a2652-9438-4c34-aaaf-8e8b275d7520",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Analysis 6] Simple Machine Learning\n",
    "#### Linear regression\n",
    "\n",
    "Change the (weighted or unweighted) *global mean* data from xarray.dataset to numpy.array format and fit a linear regression between precip and radiation (just use the monthly data from ds_new). Namely, fit y = precipitation as a function of x = radiation using linear regression. There are many [Python packages](https://towardsdatascience.com/five-regression-python-modules-that-every-data-scientist-must-know-a4e03a886853) that you can use.\n",
    "\n",
    "Visualize your predicted data along with the raw data.\n",
    "\n",
    "You are not asked to tune the regression model for accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488a906-12c2-4644-b7a1-0b7311f33113",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n",
    "x = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816eedb-8274-4bb3-938b-4faa64dbaa4b",
   "metadata": {},
   "source": [
    "Replace the precip and radiation data with their standard anomalies. Is there any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8280b7-e810-48eb-9ed2-4cbdcca6773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### to be implemented...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434a548-149e-4532-8073-10a836647bcf",
   "metadata": {},
   "source": [
    "Are you satisfied with your model and prediction? How to understand the relationship (think of physical mechanisms)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b7c4c-d04e-4571-be3b-24b4ba2a441e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
